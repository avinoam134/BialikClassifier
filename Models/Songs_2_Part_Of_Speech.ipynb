{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrqZ9TG7n-mV",
        "outputId": "37ecf20d-a116-4140-a622-c69551250c0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "106\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Open the JSON file\n",
        "with open('./../Poems_And_Outputs/all_songs_trankit_output.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "datasongs = []\n",
        "datalabels = []\n",
        "\n",
        "for song in data:\n",
        "  if not song['year'] == \"\":\n",
        "    datasongs.append(song['content'])\n",
        "    if int(song['year']) < 1900:\n",
        "      datalabels.append(0)\n",
        "    elif int(song['year']) < 1910:\n",
        "      datalabels.append(1)\n",
        "    else:\n",
        "      datalabels.append(2)\n",
        "print(len(datalabels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r70jaQQqHFB",
        "outputId": "c193de65-3070-432f-cc8f-a92ca73b3750"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['PROPN', 'ADJ', 'NOUN', 'PRON', 'PUNCT', 'NOUN', 'ADJ', 'PUNCT', 'ADP', 'NOUN', 'DET', 'NOUN', 'ADP', 'ADP', 'NOUN', 'PUNCT', 'ADP', 'NOUN', 'ADP', 'PRON', 'SCONJ', 'NOUN', 'ADV', 'VERB', 'NOUN', 'NOUN', 'ADP', 'PRON', 'ADP', 'NOUN', 'X', 'VERB', 'ADP', 'NOUN', 'PUNCT', 'PROPN', 'PUNCT', 'NOUN', 'PUNCT', 'ADJ', 'DET', 'VERB', 'PUNCT', 'NOUN', 'NOUN', 'ADJ', 'PUNCT', 'ADV', 'ADV', 'ADP', 'DET', 'NOUN', 'DET', 'ADJ', 'PUNCT', 'DET', 'ADJ', 'PUNCT', 'VERB', 'DET', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'PRON', 'PROPN', 'ADP', 'PROPN', 'ADP', 'PROPN', 'PUNCT', 'ADV', 'DET', 'NOUN', 'DET', 'ADJ', 'PUNCT', 'PROPN', 'VERB', 'PUNCT', 'VERB', 'NOUN', 'SCONJ', 'VERB', 'PUNCT', 'CCONJ', 'VERB', 'ADP', 'VERB', 'PUNCT', 'VERB', 'VERB', 'ADV', 'VERB', 'ADV', 'NOUN', 'PUNCT', 'ADV', 'VERB', 'PUNCT', 'DET', 'VERB', 'ADP', 'PRON', 'VERB', 'PUNCT', 'ADJ', 'PUNCT', 'ADJ', 'PUNCT', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'PRON', 'PROPN', 'NOUN', 'DET', 'NOUN', 'PUNCT', 'ADP', 'NOUN', 'PUNCT', 'ADP', 'NOUN', 'PUNCT', 'ADP', 'NOUN', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'PUNCT', 'SCONJ', 'VERB', 'VERB', 'ADP', 'DET', 'NOUN', 'PUNCT', 'SCONJ', 'VERB', 'ADP', 'DET', 'NOUN', 'PUNCT', 'CCONJ', 'NOUN', 'DET', 'PROPN', 'CCONJ', 'NOUN', 'DET', 'NOUN', 'PUNCT', 'VERB', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'NOUN', 'ADP', 'PRON', 'DET', 'NUM', 'PUNCT', 'DET', 'NOUN', 'DET', 'ADJ', 'PUNCT', 'DET', 'NOUN', 'PUNCT', 'DET', 'VERB', 'ADP', 'NOUN', 'VERB', 'ADP', 'NOUN', 'NOUN', 'PUNCT', 'SCONJ', 'VERB', 'CCONJ', 'VERB', 'ADP', 'NOUN', 'PUNCT', 'CCONJ', 'ADV', 'ADP', 'NOUN', 'DET', 'PROPN', 'CCONJ', 'ADV', 'DET', 'ADJ', 'PUNCT', 'CCONJ', 'NOUN', 'DET', 'ADP', 'DET', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'PUNCT', 'VERB', 'ADP', 'PRON', 'DET', 'NOUN', 'DET', 'ADJ', 'PUNCT', 'NOUN', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'PUNCT', 'NOUN', 'PUNCT', 'ADJ', 'PUNCT', 'ADP', 'PROPN', 'NOUN', 'ADP', 'PRON', 'VERB', 'NOUN', 'DET', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'PUNCT', 'ADV', 'ADP', 'ADV', 'ADV', 'ADP', 'VERB', 'DET', 'NOUN', 'SCONJ', 'VERB', 'SCONJ', 'VERB', 'ADV', 'PUNCT', 'VERB', 'NOUN', 'ADP', 'PRON', 'VERB', 'PUNCT', 'CCONJ', 'ADV', 'VERB', 'PUNCT', 'VERB', 'PROPN', 'PUNCT', 'NOUN', 'PUNCT', 'ADJ', 'PUNCT', 'NOUN', 'SCONJ', 'VERB', 'DET', 'ADP', 'NOUN', 'PUNCT', 'CCONJ', 'ADV', 'ADP', 'VERB', 'NOUN', 'ADP', 'PRON', 'VERB', 'PUNCT', 'VERB', 'ADJ', 'SCONJ', 'NOUN', 'ADP', 'PRON', 'ADP', 'NOUN', 'PUNCT', 'NOUN', 'ADP', 'PROPN', 'ADV', 'PUNCT', 'CCONJ', 'NOUN', 'DET', 'NOUN', 'PUNCT', 'SCONJ', 'VERB', 'ADP', 'NOUN', 'PUNCT', 'VERB', 'PROPN', 'DET', 'NOUN', 'PUNCT', 'PUNCT', 'ADV', 'VERB', 'ADP', 'ADV', 'NOUN', 'CCONJ', 'VERB', 'ADP', 'NOUN', 'NOUN', 'ADP', 'PRON', 'VERB', 'DET', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'PUNCT', 'CCONJ', 'PRON', 'ADV', 'VERB', 'ADP', 'PRON', 'PUNCT', 'VERB', 'ADJ', 'PUNCT', 'ADP', 'NOUN', 'ADV', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'ADP', 'NOUN', 'NOUN', 'VERB', 'ADV', 'ADP', 'NOUN', 'NOUN', 'ADP', 'PRON', 'PUNCT', 'ADV', 'NOUN', 'PUNCT', 'ADV', 'NOUN', 'PUNCT', 'DET', 'VERB', 'DET', 'NOUN', 'PUNCT', 'SCONJ', 'ADV', 'PRON', 'ADP', 'NOUN', 'DET', 'NOUN', 'VERB', 'PUNCT', 'NOUN', 'PUNCT', 'PUNCT', 'CCONJ', 'PUNCT', 'VERB', 'ADV', 'VERB', 'ADP', 'DET', 'NOUN', 'DET', 'VERB', 'PUNCT', 'ADP', 'DET', 'NOUN', 'VERB', 'CCONJ', 'ADP', 'DET', 'NOUN', 'PUNCT', 'VERB', 'PUNCT', 'ADJ', 'PUNCT', 'ADV', 'ADP', 'NOUN', 'PUNCT', 'ADP', 'NOUN', 'PUNCT', 'VERB', 'PUNCT', 'SCONJ', 'VERB', 'ADP', 'ADP', 'PROPN', 'PUNCT', 'CCONJ', 'PROPN', 'PROPN', 'PUNCT', 'ADV', 'ADV', 'ADP', 'DET', 'NOUN', 'PUNCT', 'NOUN', 'NOUN', 'PUNCT', 'VERB', 'PUNCT', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'PUNCT', 'CCONJ', 'ADV', 'NOUN', 'CCONJ', 'NOUN', 'ADP', 'NOUN', 'ADP', 'PRON', 'VERB', 'PUNCT', 'ADV', 'PRON', 'VERB', 'ADP', 'PROPN', 'PUNCT', 'ADV', 'VERB', 'PUNCT', 'VERB', 'NOUN', 'NOUN', 'PUNCT', 'ADV', 'VERB', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'ADV', 'VERB', 'PRON', 'DET', 'NOUN', 'PUNCT', 'ADV', 'NOUN', 'PRON', 'DET', 'NOUN', 'PUNCT', 'CCONJ', 'ADV', 'DET', 'NOUN', 'VERB', 'ADP', 'DET', 'ADJ', 'PUNCT', 'NOUN', 'ADJ', 'NOUN', 'PRON', 'PUNCT', 'NOUN', 'ADP', 'PRON', 'DET', 'ADJ', 'PUNCT', 'PROPN', 'PROPN', 'NOUN', 'ADP', 'PRON', 'CCONJ', 'PROPN', 'PUNCT']\n"
          ]
        }
      ],
      "source": [
        "def word_to_list_of_part_of_speech(word):\n",
        "  ans = []\n",
        "  if word.get('upos') is not None:\n",
        "    ans.append(word['upos'])\n",
        "    return ans\n",
        "  subwords = word['expanded']\n",
        "  for subword in subwords:\n",
        "    ans.extend(word_to_list_of_part_of_speech(subword))\n",
        "  return ans\n",
        "\n",
        "\n",
        "\n",
        "def song_to_list_of_part_of_speech(sentences):\n",
        "  ans = []\n",
        "  for part in sentences:\n",
        "    tokens = part['tokens']\n",
        "    for word in tokens:\n",
        "      ans.extend(word_to_list_of_part_of_speech(word))\n",
        "  return ans\n",
        "\n",
        "\n",
        "print(song_to_list_of_part_of_speech(data[0]['content']['sentences']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxmLMnumvjQj",
        "outputId": "90510e61-a370-42da-9c11-356b69b23723"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "106\n"
          ]
        }
      ],
      "source": [
        "allSongsByPartOfSpeech = []\n",
        "\n",
        "for song in datasongs:\n",
        "  allSongsByPartOfSpeech.append(song_to_list_of_part_of_speech(song['sentences']))\n",
        "\n",
        "print(len(allSongsByPartOfSpeech))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azhOqxETxMwQ",
        "outputId": "3042ceb5-a566-4f2a-dd3a-eb97e03cb1e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['PROPN', 'ADJ', 'NOUN', 'PRON', 'PUNCT', 'ADP', 'DET', 'SCONJ', 'ADV', 'VERB', 'X', 'CCONJ', 'NUM', 'AUX']\n"
          ]
        }
      ],
      "source": [
        "typesPartOfSpeech = []\n",
        "\n",
        "for song in allSongsByPartOfSpeech:\n",
        "  for part in song:\n",
        "    if part not in typesPartOfSpeech:\n",
        "      typesPartOfSpeech.append(part)\n",
        "\n",
        "print(typesPartOfSpeech)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOj8JMRY1yZ0",
        "outputId": "78ce96aa-faa9-4dc2-9988-6aba839be4a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "106\n"
          ]
        }
      ],
      "source": [
        "allSongsByPartOfSpeechIndex = []\n",
        "for song in allSongsByPartOfSpeech:\n",
        "  songByIndex = []\n",
        "  for part in song:\n",
        "    songByIndex.append(typesPartOfSpeech.index(part))\n",
        "  allSongsByPartOfSpeechIndex.append(songByIndex)\n",
        "\n",
        "print(len(allSongsByPartOfSpeechIndex))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipNOOH7g7A0A",
        "outputId": "e4b35060-a98c-4d69-e596-a65217082292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5909090909090909\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "padding_value = 16\n",
        "test_size = 0.2\n",
        "random_state = 42\n",
        "\n",
        "# Sample data: list of lists of integers\n",
        "data = allSongsByPartOfSpeechIndex\n",
        "labels = datalabels\n",
        "\n",
        "# Find the maximum length of the lists\n",
        "max_length = max(len(x) for x in data)\n",
        "\n",
        "# Pad the lists with 16s to make them of equal length\n",
        "padded_data = [x + [padding_value] * (max_length - len(x)) for x in data]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_data, labels, test_size=test_size, random_state=random_state)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zhws0by-Fu6",
        "outputId": "fa197a6c-a544-415d-ccf5-9b9f32bb00df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3/3 [==============================] - 6s 2s/step - loss: 1.0987 - accuracy: 0.4133 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 3s 870ms/step - loss: 1.0904 - accuracy: 0.3867 - val_loss: 1.0993 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 3s 878ms/step - loss: 1.0832 - accuracy: 0.4267 - val_loss: 1.1005 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 3s 894ms/step - loss: 1.0753 - accuracy: 0.4267 - val_loss: 1.1025 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 3s 895ms/step - loss: 1.0683 - accuracy: 0.4267 - val_loss: 1.1057 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 4s 1s/step - loss: 1.0603 - accuracy: 0.4267 - val_loss: 1.1111 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 3s 890ms/step - loss: 1.0509 - accuracy: 0.4267 - val_loss: 1.1207 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 3s 892ms/step - loss: 1.0389 - accuracy: 0.4267 - val_loss: 1.1374 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 3s 880ms/step - loss: 1.0280 - accuracy: 0.4267 - val_loss: 1.1774 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 4s 1s/step - loss: 1.0259 - accuracy: 0.4267 - val_loss: 1.2382 - val_accuracy: 0.3333\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.9961 - accuracy: 0.4545\n",
            "Test Accuracy: 0.4545454680919647\n",
            "1/1 [==============================] - 1s 900ms/step\n",
            "Accuracy: 0.45454545454545453\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Hyperparameters\n",
        "padding_value = 16\n",
        "test_size = 0.2\n",
        "random_state = 42\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "# Sample data: list of lists of integers\n",
        "data = allSongsByPartOfSpeechIndex\n",
        "labels = datalabels\n",
        "\n",
        "# Find the maximum length of the lists\n",
        "max_length = max(len(x) for x in data)\n",
        "\n",
        "# Pad the lists with 16s to make them of equal length\n",
        "padded_data = [x + [padding_value] * (max_length - len(x)) for x in data]\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X = np.array(padded_data)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=max_length+1, output_dim=16, input_length=max_length),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj-nMVfoCif5",
        "outputId": "488c9261-3f03-405f-8a6f-2c512235d723"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3/3 [==============================] - 10s 280ms/step - loss: 1.0978 - accuracy: 0.3467 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.0913 - accuracy: 0.4267 - val_loss: 1.0994 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 1.0824 - accuracy: 0.4133 - val_loss: 1.1012 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.0715 - accuracy: 0.4133 - val_loss: 1.1080 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.0525 - accuracy: 0.4133 - val_loss: 1.1388 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 1.0287 - accuracy: 0.4267 - val_loss: 1.2774 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0406 - accuracy: 0.4267 - val_loss: 1.2180 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.0231 - accuracy: 0.4267 - val_loss: 1.2285 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.0257 - accuracy: 0.3467 - val_loss: 1.2217 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 1.0241 - accuracy: 0.4133 - val_loss: 1.2182 - val_accuracy: 0.3333\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9988 - accuracy: 0.4091\n",
            "Test Accuracy: 0.40909090638160706\n",
            "1/1 [==============================] - 0s 397ms/step\n",
            "Accuracy: 0.4090909090909091\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Hyperparameters\n",
        "padding_value = 16\n",
        "test_size = 0.2\n",
        "random_state = 42\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "num_layers = 50\n",
        "\n",
        "# Sample data: list of lists of integers\n",
        "data = allSongsByPartOfSpeechIndex\n",
        "labels = datalabels\n",
        "\n",
        "# Find the maximum length of the lists\n",
        "max_length = max(len(x) for x in data)\n",
        "\n",
        "# Pad the lists with 16s to make them of equal length\n",
        "padded_data = [x + [padding_value] * (max_length - len(x)) for x in data]\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X = np.array(padded_data)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "# Build the deep neural network model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(max_length,)))\n",
        "for _ in range(num_layers):\n",
        "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
